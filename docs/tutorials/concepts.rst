Basic Concepts in ZhuSuan
=========================

.. _dist:

Distribution
------------

Distributions are basic functionalities for building probabilistic models.
The :class:`~zhusuan.distributions.base.Distribution` class is the base class
for various probabilistic distributions which support batch inputs, generating
batches of samples and evaluate probabilities at batches of given values.

The list of all available distributions can be found on these pages:

* :mod:`univariate distributions <zhusuan.distributions.univariate>`
* :mod:`multivariate distributions <zhusuan.distributions.multivariate>`

We can create a univariate Normal distribution in ZhuSuan by::

    >>> import zhusuan as zs
    >>> a = zs.distributions.Normal(mean=0., logstd=0.)

The typical input shape for a :class:`~zhusuan.distributions.base.Distribution`
is like ``batch_shape + input_shape``. where ``input_shape`` represents the
shape of a non-batch input parameter;
:attr:`~.Distribution.batch_shape` represents how many independent inputs are
fed into the distribution. In general, distributions support broadcasting for
inputs.

Samples can be generated by calling
:meth:`~zhusuan.distributions.base.Distribution.sample` method of distribution
objects. The shape is ``([n_samples] + )batch_shape + value_shape``.
The first additional axis is omitted only when passed `n_samples` is None
(by default), in which case one sample is generated. ``value_shape`` is the
non-batch value shape of the distribution. For a univariate distribution,
its ``value_shape`` is [].

An example of univariate distributions
(:class:`~zhusuan.distributions.univariate.Normal`)::

    >>> import tensorflow as tf
    >>> _ = tf.InteractiveSession()

    >>> b = zs.distributions.Normal([[-1., 1.], [0., -2.]], [0., 1.])

    >>> b.batch_shape.eval()
    array([2, 2], dtype=int32)

    >>> b.value_shape.eval()
    array([], dtype=int32)

    >>> tf.shape(b.sample()).eval()
    array([2, 2], dtype=int32)

    >>> tf.shape(b.sample(1)).eval()
    array([1, 2, 2], dtype=int32)

    >>> tf.shape(b.sample(10)).eval()
    array([10,  2,  2], dtype=int32)

and an example of multivariate distributions
(:class:`~zhusuan.distributions.multivariate.OnehotCategorical`)::

    >>> c = zs.distributions.OnehotCategorical([[0., 1., -1.],
    ...                                         [2., 3., 4.]])

    >>> c.batch_shape.eval()
    array([2], dtype=int32)

    >>> c.value_shape.eval()
    array([3], dtype=int32)

    >>> tf.shape(c.sample()).eval()
    array([2, 3], dtype=int32)

    >>> tf.shape(c.sample(1)).eval()
    array([1, 2, 3], dtype=int32)

    >>> tf.shape(c.sample(10)).eval()
    array([10,  2,  3], dtype=int32)

There are cases where a batch of random variables are grouped into a
single event so that their probabilities can be computed together. This
is achieved by setting `group_ndims` argument, which defaults to 0.
The last `group_ndims` number of axes in
:attr:`~.Distribution.batch_shape` are grouped into a single event.
For example, ``Normal(..., group_ndims=1)`` will
set the last axis of its :attr:`~.Distribution.batch_shape` to a single event,
i.e., a multivariate Normal with identity covariance matrix.

The log probability density (mass) function can be evaluated by passing given
values to :meth:`~zhusuan.distributions.base.Distribution.log_prob` method of
distribution objects.
In that case, the given Tensor should be
broadcastable to shape ``(... + )batch_shape + value_shape``. The returned
Tensor has shape ``(... + )batch_shape[:-group_ndims]``. For example::

    >>> d = zs.distributions.Normal([[-1., 1.], [0., -2.]], 0.,
    ...                             group_ndims=1)

    >>> d.log_prob(0.).eval()
    array([-2.83787704, -3.83787727], dtype=float32)

    >>> e = zs.distributions.Normal(tf.zeros([2, 1, 3]), 0.,
    ...                             group_ndims=2)

    >>> tf.shape(e.log_prob(tf.zeros([5, 1, 1, 3]))).eval()
    array([5, 2], dtype=int32)

.. _bayesian-net:

BayesianNet
-----------

In ZhuSuan we support building probabilistic models as Bayesian networks, i.e.,
directed graphical models. We use a simple Bayesian linear regression example
to illustrate this. The generative process of the model is

.. math::

    w \sim N(0, \alpha^2 I)

    y \sim N(w^\top x, \beta^2)

where :math:`x` denotes the input feature in the linear regression. We apply a
Bayesian treatment and assume a Normal prior distribution of the regression
weights :math:`w`.

To define the model, the first step is to construct a
:class:`~zhusuan.framework.bn.BayesianNet` instance::

    bn = zs.BayesianNet()

A Bayesian network describes the dependency structure of the joint
distribution over a set of random variables as directed graphs. To support
this, a :class:`~zhusuan.framework.bn.BayesianNet` instance can keep two kinds
of nodes:

* Stochastic nodes. They are random variables in graphical models.
  The ``w`` node can be constructed as

::

    w = bn.normal("w", tf.zeros([x.shape[-1]], std=alpha)

Here ``w`` is a :class:`~zhusuan.framework.bn.StochasticTensor` that follows
the :class:`~zhusuan.distributions.univariate.Normal` distribution. For any
distribution available in :mod:`zhusuan.distributions`, we can find
a method of :class:`BayesianNet` for creating the corresponding stochastic
node. The returned :class:`~zhusuan.framework.bn.StochasticTensor` instances
are Tensor-like, which means that you can mix it with almost any Tensorflow
primitives, for example, the predicted mean of the linear regression is an
inner product between ``w`` and input ``x``::

    y_mean = tf.reduce_sum(w * x, axis=-1)

* Deterministic nodes. As the above code shows, deterministic nodes can be
  constructed directly with Tensorflow operations, and in this way
  :class:`~zhusuan.framework.bn.BayesianNet` does not keep track of them.
  However, in some cases it's convenient to enable the tracking by the
  :meth:`~zhusuan.framework.BayesianNet.deterministic` method::

    y_mean = bn.deterministic("y_mean", tf.reduce_sum(w * x, axis=-1))

  This allow you to fetch the ``y_mean`` Tensor from ``bn`` whenever you want
  it.

The full code of building a Bayesian linear regression model is like::

    def bayesian_linear_regression(x, alpha, beta):
        bn = zs.BayesianNet()
        w = bn.normal("w", mean=0., std=alpha)
        y_mean = tf.reduce_sum(w * x, axis=-1)
        y = bn.normal("y", y_mean, std=beta)
        return model

To observe any stochastic nodes in the network, pass a dictionary mapping of
``(name, Tensor)`` pairs when constructing
:class:`~zhusuan.model.base.BayesianNet`. This will assign observed values
to corresponding :class:`~zhusuan.model.base.StochasticTensor` s. For example::

    def bayesian_linear_regression(observed, x, alpha, beta):
        with zs.BayesianNet(observed=observed) as model:
            w = zs.Normal('w', mean=0., logstd=tf.log(alpha)
            y_mean = tf.reduce_sum(tf.expand_dims(w, 0) * x, 1)
            y = zs.Normal('y', y_mean, tf.log(beta))
        return model

    model = bayesian_linear_regression({'w': w_obs}, ...)

will set ``w`` to be observed. The result is that ``y_mean`` is computed
from the observed value of ``w`` (``w_obs``) instead of the samples of ``w``.
Calling the above function with different `observed` arguments instantiates the
:class:`~zhusuan.model.base.BayesianNet` with different observations, which
is a common behaviour for probabilistic graphical models.

.. Note::

    The observation passed must have the same type and shape as the
    `StochasticTensor`.

If there are
tensorflow `Variables <https://www.tensorflow.org/api_docs/python/tf/Variable>`_
created in this function, you may want to reuse it. ZhuSuan provides an easy
way to do this. You could just add a decorator to the function::

    @zs.reuse(scope="model")
    def bayesian_linear_regression(observed, x, alpha, beta):
        ...

See :func:`~zhusuan.model.base.reuse` for details.

After construction, :class:`~zhusuan.model.base.BayesianNet` supports queries
on the network::

    # get samples of random variable y following generative process
    # in the network
    model.outputs('y')

    # because w is observed in this case, its observed value will be
    # returned
    model.outputs('w')

    # get local log probability values of w and y, which returns
    # log p(w) and log p(y|w, x)
    model.local_log_prob(['w', 'y'])

    # query many quantities at the same time
    model.query('w', outputs=True, local_log_prob=True)

.. bibliography:: ../refs.bib
    :style: unsrtalpha
    :keyprefix: concepts-